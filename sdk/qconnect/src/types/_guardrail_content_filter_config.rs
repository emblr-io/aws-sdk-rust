// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Contains filter strengths for harmful content. AI Guardrail's support the following content filters to detect and filter harmful user inputs and FM-generated outputs.</p>
/// <ul>
/// <li>
/// <p><b>Hate</b>: Describes input prompts and model responses that discriminate, criticize, insult, denounce, or dehumanize a person or group on the basis of an identity (such as race, ethnicity, gender, religion, sexual orientation, ability, and national origin).</p></li>
/// <li>
/// <p><b>Insults</b>: Describes input prompts and model responses that includes demeaning, humiliating, mocking, insulting, or belittling language. This type of language is also labeled as bullying.</p></li>
/// <li>
/// <p><b>Sexual</b>: Describes input prompts and model responses that indicates sexual interest, activity, or arousal using direct or indirect references to body parts, physical traits, or sex.</p></li>
/// <li>
/// <p><b>Violence</b>: Describes input prompts and model responses that includes glorification of, or threats to inflict physical pain, hurt, or injury toward a person, group, or thing.</p></li>
/// </ul>
/// <p>Content filtering depends on the confidence classification of user inputs and FM responses across each of the four harmful categories. All input and output statements are classified into one of four confidence levels (NONE, LOW, MEDIUM, HIGH) for each harmful category. For example, if a statement is classified as <i>Hate</i> with HIGH confidence, the likelihood of the statement representing hateful content is high. A single statement can be classified across multiple categories with varying confidence levels. For example, a single statement can be classified as <i>Hate</i> with HIGH confidence, <i> Insults</i> with LOW confidence, <i>Sexual</i> with NONE confidence, and <i>Violence</i> with MEDIUM confidence.</p>
#[non_exhaustive]
#[cfg_attr(feature = "serde-serialize", derive(::serde::Serialize))]
#[cfg_attr(feature = "serde-deserialize", derive(::serde::Deserialize))]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq)]
pub struct GuardrailContentFilterConfig {
    /// <p>The harmful category that the content filter is applied to.</p>
    pub r#type: crate::types::GuardrailContentFilterType,
    /// <p>The strength of the content filter to apply to prompts. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    pub input_strength: crate::types::GuardrailFilterStrength,
    /// <p>The strength of the content filter to apply to model responses. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    pub output_strength: crate::types::GuardrailFilterStrength,
}
impl GuardrailContentFilterConfig {
    /// <p>The harmful category that the content filter is applied to.</p>
    pub fn r#type(&self) -> &crate::types::GuardrailContentFilterType {
        &self.r#type
    }
    /// <p>The strength of the content filter to apply to prompts. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    pub fn input_strength(&self) -> &crate::types::GuardrailFilterStrength {
        &self.input_strength
    }
    /// <p>The strength of the content filter to apply to model responses. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    pub fn output_strength(&self) -> &crate::types::GuardrailFilterStrength {
        &self.output_strength
    }
}
impl ::std::fmt::Debug for GuardrailContentFilterConfig {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("GuardrailContentFilterConfig");
        formatter.field("r#type", &"*** Sensitive Data Redacted ***");
        formatter.field("input_strength", &"*** Sensitive Data Redacted ***");
        formatter.field("output_strength", &"*** Sensitive Data Redacted ***");
        formatter.finish()
    }
}
impl GuardrailContentFilterConfig {
    /// Creates a new builder-style object to manufacture [`GuardrailContentFilterConfig`](crate::types::GuardrailContentFilterConfig).
    pub fn builder() -> crate::types::builders::GuardrailContentFilterConfigBuilder {
        crate::types::builders::GuardrailContentFilterConfigBuilder::default()
    }
}

/// A builder for [`GuardrailContentFilterConfig`](crate::types::GuardrailContentFilterConfig).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default)]
#[non_exhaustive]
pub struct GuardrailContentFilterConfigBuilder {
    pub(crate) r#type: ::std::option::Option<crate::types::GuardrailContentFilterType>,
    pub(crate) input_strength: ::std::option::Option<crate::types::GuardrailFilterStrength>,
    pub(crate) output_strength: ::std::option::Option<crate::types::GuardrailFilterStrength>,
}
impl GuardrailContentFilterConfigBuilder {
    /// <p>The harmful category that the content filter is applied to.</p>
    /// This field is required.
    pub fn r#type(mut self, input: crate::types::GuardrailContentFilterType) -> Self {
        self.r#type = ::std::option::Option::Some(input);
        self
    }
    /// <p>The harmful category that the content filter is applied to.</p>
    pub fn set_type(mut self, input: ::std::option::Option<crate::types::GuardrailContentFilterType>) -> Self {
        self.r#type = input;
        self
    }
    /// <p>The harmful category that the content filter is applied to.</p>
    pub fn get_type(&self) -> &::std::option::Option<crate::types::GuardrailContentFilterType> {
        &self.r#type
    }
    /// <p>The strength of the content filter to apply to prompts. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    /// This field is required.
    pub fn input_strength(mut self, input: crate::types::GuardrailFilterStrength) -> Self {
        self.input_strength = ::std::option::Option::Some(input);
        self
    }
    /// <p>The strength of the content filter to apply to prompts. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    pub fn set_input_strength(mut self, input: ::std::option::Option<crate::types::GuardrailFilterStrength>) -> Self {
        self.input_strength = input;
        self
    }
    /// <p>The strength of the content filter to apply to prompts. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    pub fn get_input_strength(&self) -> &::std::option::Option<crate::types::GuardrailFilterStrength> {
        &self.input_strength
    }
    /// <p>The strength of the content filter to apply to model responses. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    /// This field is required.
    pub fn output_strength(mut self, input: crate::types::GuardrailFilterStrength) -> Self {
        self.output_strength = ::std::option::Option::Some(input);
        self
    }
    /// <p>The strength of the content filter to apply to model responses. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    pub fn set_output_strength(mut self, input: ::std::option::Option<crate::types::GuardrailFilterStrength>) -> Self {
        self.output_strength = input;
        self
    }
    /// <p>The strength of the content filter to apply to model responses. As you increase the filter strength, the likelihood of filtering harmful content increases and the probability of seeing harmful content in your application reduces.</p>
    pub fn get_output_strength(&self) -> &::std::option::Option<crate::types::GuardrailFilterStrength> {
        &self.output_strength
    }
    /// Consumes the builder and constructs a [`GuardrailContentFilterConfig`](crate::types::GuardrailContentFilterConfig).
    /// This method will fail if any of the following fields are not set:
    /// - [`r#type`](crate::types::builders::GuardrailContentFilterConfigBuilder::type)
    /// - [`input_strength`](crate::types::builders::GuardrailContentFilterConfigBuilder::input_strength)
    /// - [`output_strength`](crate::types::builders::GuardrailContentFilterConfigBuilder::output_strength)
    pub fn build(self) -> ::std::result::Result<crate::types::GuardrailContentFilterConfig, ::aws_smithy_types::error::operation::BuildError> {
        ::std::result::Result::Ok(crate::types::GuardrailContentFilterConfig {
            r#type: self.r#type.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "r#type",
                    "r#type was not specified but it is required when building GuardrailContentFilterConfig",
                )
            })?,
            input_strength: self.input_strength.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "input_strength",
                    "input_strength was not specified but it is required when building GuardrailContentFilterConfig",
                )
            })?,
            output_strength: self.output_strength.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "output_strength",
                    "output_strength was not specified but it is required when building GuardrailContentFilterConfig",
                )
            })?,
        })
    }
}
impl ::std::fmt::Debug for GuardrailContentFilterConfigBuilder {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("GuardrailContentFilterConfigBuilder");
        formatter.field("r#type", &"*** Sensitive Data Redacted ***");
        formatter.field("input_strength", &"*** Sensitive Data Redacted ***");
        formatter.field("output_strength", &"*** Sensitive Data Redacted ***");
        formatter.finish()
    }
}
