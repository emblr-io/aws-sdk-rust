// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[cfg_attr(feature = "serde-serialize", derive(::serde::Serialize))]
#[cfg_attr(feature = "serde-deserialize", derive(::serde::Deserialize))]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct CreateOptimizationJobInput {
    /// <p>A custom name for the new optimization job.</p>
    pub optimization_job_name: ::std::option::Option<::std::string::String>,
    /// <p>The Amazon Resource Name (ARN) of an IAM role that enables Amazon SageMaker AI to perform tasks on your behalf.</p>
    /// <p>During model optimization, Amazon SageMaker AI needs your permission to:</p>
    /// <ul>
    /// <li>
    /// <p>Read input data from an S3 bucket</p></li>
    /// <li>
    /// <p>Write model artifacts to an S3 bucket</p></li>
    /// <li>
    /// <p>Write logs to Amazon CloudWatch Logs</p></li>
    /// <li>
    /// <p>Publish metrics to Amazon CloudWatch</p></li>
    /// </ul>
    /// <p>You grant permissions for all of these tasks to an IAM role. To pass this role to Amazon SageMaker AI, the caller of this API must have the <code>iam:PassRole</code> permission. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html">Amazon SageMaker AI Roles.</a></p>
    pub role_arn: ::std::option::Option<::std::string::String>,
    /// <p>The location of the source model to optimize with an optimization job.</p>
    pub model_source: ::std::option::Option<crate::types::OptimizationJobModelSource>,
    /// <p>The type of instance that hosts the optimized model that you create with the optimization job.</p>
    pub deployment_instance_type: ::std::option::Option<crate::types::OptimizationJobDeploymentInstanceType>,
    /// <p>The environment variables to set in the model container.</p>
    pub optimization_environment: ::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>>,
    /// <p>Settings for each of the optimization techniques that the job applies.</p>
    pub optimization_configs: ::std::option::Option<::std::vec::Vec<crate::types::OptimizationConfig>>,
    /// <p>Details for where to store the optimized model that you create with the optimization job.</p>
    pub output_config: ::std::option::Option<crate::types::OptimizationJobOutputConfig>,
    /// <p>Specifies a limit to how long a job can run. When the job reaches the time limit, SageMaker ends the job. Use this API to cap costs.</p>
    /// <p>To stop a training job, SageMaker sends the algorithm the <code>SIGTERM</code> signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost.</p>
    /// <p>The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with <code>CreateModel</code>.</p><note>
    /// <p>The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.</p>
    /// </note>
    pub stopping_condition: ::std::option::Option<crate::types::StoppingCondition>,
    /// <p>A list of key-value pairs associated with the optimization job. For more information, see <a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">Tagging Amazon Web Services resources</a> in the <i>Amazon Web Services General Reference Guide</i>.</p>
    pub tags: ::std::option::Option<::std::vec::Vec<crate::types::Tag>>,
    /// <p>A VPC in Amazon VPC that your optimized model has access to.</p>
    pub vpc_config: ::std::option::Option<crate::types::OptimizationVpcConfig>,
}
impl CreateOptimizationJobInput {
    /// <p>A custom name for the new optimization job.</p>
    pub fn optimization_job_name(&self) -> ::std::option::Option<&str> {
        self.optimization_job_name.as_deref()
    }
    /// <p>The Amazon Resource Name (ARN) of an IAM role that enables Amazon SageMaker AI to perform tasks on your behalf.</p>
    /// <p>During model optimization, Amazon SageMaker AI needs your permission to:</p>
    /// <ul>
    /// <li>
    /// <p>Read input data from an S3 bucket</p></li>
    /// <li>
    /// <p>Write model artifacts to an S3 bucket</p></li>
    /// <li>
    /// <p>Write logs to Amazon CloudWatch Logs</p></li>
    /// <li>
    /// <p>Publish metrics to Amazon CloudWatch</p></li>
    /// </ul>
    /// <p>You grant permissions for all of these tasks to an IAM role. To pass this role to Amazon SageMaker AI, the caller of this API must have the <code>iam:PassRole</code> permission. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html">Amazon SageMaker AI Roles.</a></p>
    pub fn role_arn(&self) -> ::std::option::Option<&str> {
        self.role_arn.as_deref()
    }
    /// <p>The location of the source model to optimize with an optimization job.</p>
    pub fn model_source(&self) -> ::std::option::Option<&crate::types::OptimizationJobModelSource> {
        self.model_source.as_ref()
    }
    /// <p>The type of instance that hosts the optimized model that you create with the optimization job.</p>
    pub fn deployment_instance_type(&self) -> ::std::option::Option<&crate::types::OptimizationJobDeploymentInstanceType> {
        self.deployment_instance_type.as_ref()
    }
    /// <p>The environment variables to set in the model container.</p>
    pub fn optimization_environment(&self) -> ::std::option::Option<&::std::collections::HashMap<::std::string::String, ::std::string::String>> {
        self.optimization_environment.as_ref()
    }
    /// <p>Settings for each of the optimization techniques that the job applies.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.optimization_configs.is_none()`.
    pub fn optimization_configs(&self) -> &[crate::types::OptimizationConfig] {
        self.optimization_configs.as_deref().unwrap_or_default()
    }
    /// <p>Details for where to store the optimized model that you create with the optimization job.</p>
    pub fn output_config(&self) -> ::std::option::Option<&crate::types::OptimizationJobOutputConfig> {
        self.output_config.as_ref()
    }
    /// <p>Specifies a limit to how long a job can run. When the job reaches the time limit, SageMaker ends the job. Use this API to cap costs.</p>
    /// <p>To stop a training job, SageMaker sends the algorithm the <code>SIGTERM</code> signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost.</p>
    /// <p>The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with <code>CreateModel</code>.</p><note>
    /// <p>The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.</p>
    /// </note>
    pub fn stopping_condition(&self) -> ::std::option::Option<&crate::types::StoppingCondition> {
        self.stopping_condition.as_ref()
    }
    /// <p>A list of key-value pairs associated with the optimization job. For more information, see <a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">Tagging Amazon Web Services resources</a> in the <i>Amazon Web Services General Reference Guide</i>.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.tags.is_none()`.
    pub fn tags(&self) -> &[crate::types::Tag] {
        self.tags.as_deref().unwrap_or_default()
    }
    /// <p>A VPC in Amazon VPC that your optimized model has access to.</p>
    pub fn vpc_config(&self) -> ::std::option::Option<&crate::types::OptimizationVpcConfig> {
        self.vpc_config.as_ref()
    }
}
impl CreateOptimizationJobInput {
    /// Creates a new builder-style object to manufacture [`CreateOptimizationJobInput`](crate::operation::create_optimization_job::CreateOptimizationJobInput).
    pub fn builder() -> crate::operation::create_optimization_job::builders::CreateOptimizationJobInputBuilder {
        crate::operation::create_optimization_job::builders::CreateOptimizationJobInputBuilder::default()
    }
}

/// A builder for [`CreateOptimizationJobInput`](crate::operation::create_optimization_job::CreateOptimizationJobInput).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
#[non_exhaustive]
pub struct CreateOptimizationJobInputBuilder {
    pub(crate) optimization_job_name: ::std::option::Option<::std::string::String>,
    pub(crate) role_arn: ::std::option::Option<::std::string::String>,
    pub(crate) model_source: ::std::option::Option<crate::types::OptimizationJobModelSource>,
    pub(crate) deployment_instance_type: ::std::option::Option<crate::types::OptimizationJobDeploymentInstanceType>,
    pub(crate) optimization_environment: ::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>>,
    pub(crate) optimization_configs: ::std::option::Option<::std::vec::Vec<crate::types::OptimizationConfig>>,
    pub(crate) output_config: ::std::option::Option<crate::types::OptimizationJobOutputConfig>,
    pub(crate) stopping_condition: ::std::option::Option<crate::types::StoppingCondition>,
    pub(crate) tags: ::std::option::Option<::std::vec::Vec<crate::types::Tag>>,
    pub(crate) vpc_config: ::std::option::Option<crate::types::OptimizationVpcConfig>,
}
impl CreateOptimizationJobInputBuilder {
    /// <p>A custom name for the new optimization job.</p>
    /// This field is required.
    pub fn optimization_job_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.optimization_job_name = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>A custom name for the new optimization job.</p>
    pub fn set_optimization_job_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.optimization_job_name = input;
        self
    }
    /// <p>A custom name for the new optimization job.</p>
    pub fn get_optimization_job_name(&self) -> &::std::option::Option<::std::string::String> {
        &self.optimization_job_name
    }
    /// <p>The Amazon Resource Name (ARN) of an IAM role that enables Amazon SageMaker AI to perform tasks on your behalf.</p>
    /// <p>During model optimization, Amazon SageMaker AI needs your permission to:</p>
    /// <ul>
    /// <li>
    /// <p>Read input data from an S3 bucket</p></li>
    /// <li>
    /// <p>Write model artifacts to an S3 bucket</p></li>
    /// <li>
    /// <p>Write logs to Amazon CloudWatch Logs</p></li>
    /// <li>
    /// <p>Publish metrics to Amazon CloudWatch</p></li>
    /// </ul>
    /// <p>You grant permissions for all of these tasks to an IAM role. To pass this role to Amazon SageMaker AI, the caller of this API must have the <code>iam:PassRole</code> permission. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html">Amazon SageMaker AI Roles.</a></p>
    /// This field is required.
    pub fn role_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.role_arn = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of an IAM role that enables Amazon SageMaker AI to perform tasks on your behalf.</p>
    /// <p>During model optimization, Amazon SageMaker AI needs your permission to:</p>
    /// <ul>
    /// <li>
    /// <p>Read input data from an S3 bucket</p></li>
    /// <li>
    /// <p>Write model artifacts to an S3 bucket</p></li>
    /// <li>
    /// <p>Write logs to Amazon CloudWatch Logs</p></li>
    /// <li>
    /// <p>Publish metrics to Amazon CloudWatch</p></li>
    /// </ul>
    /// <p>You grant permissions for all of these tasks to an IAM role. To pass this role to Amazon SageMaker AI, the caller of this API must have the <code>iam:PassRole</code> permission. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html">Amazon SageMaker AI Roles.</a></p>
    pub fn set_role_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.role_arn = input;
        self
    }
    /// <p>The Amazon Resource Name (ARN) of an IAM role that enables Amazon SageMaker AI to perform tasks on your behalf.</p>
    /// <p>During model optimization, Amazon SageMaker AI needs your permission to:</p>
    /// <ul>
    /// <li>
    /// <p>Read input data from an S3 bucket</p></li>
    /// <li>
    /// <p>Write model artifacts to an S3 bucket</p></li>
    /// <li>
    /// <p>Write logs to Amazon CloudWatch Logs</p></li>
    /// <li>
    /// <p>Publish metrics to Amazon CloudWatch</p></li>
    /// </ul>
    /// <p>You grant permissions for all of these tasks to an IAM role. To pass this role to Amazon SageMaker AI, the caller of this API must have the <code>iam:PassRole</code> permission. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html">Amazon SageMaker AI Roles.</a></p>
    pub fn get_role_arn(&self) -> &::std::option::Option<::std::string::String> {
        &self.role_arn
    }
    /// <p>The location of the source model to optimize with an optimization job.</p>
    /// This field is required.
    pub fn model_source(mut self, input: crate::types::OptimizationJobModelSource) -> Self {
        self.model_source = ::std::option::Option::Some(input);
        self
    }
    /// <p>The location of the source model to optimize with an optimization job.</p>
    pub fn set_model_source(mut self, input: ::std::option::Option<crate::types::OptimizationJobModelSource>) -> Self {
        self.model_source = input;
        self
    }
    /// <p>The location of the source model to optimize with an optimization job.</p>
    pub fn get_model_source(&self) -> &::std::option::Option<crate::types::OptimizationJobModelSource> {
        &self.model_source
    }
    /// <p>The type of instance that hosts the optimized model that you create with the optimization job.</p>
    /// This field is required.
    pub fn deployment_instance_type(mut self, input: crate::types::OptimizationJobDeploymentInstanceType) -> Self {
        self.deployment_instance_type = ::std::option::Option::Some(input);
        self
    }
    /// <p>The type of instance that hosts the optimized model that you create with the optimization job.</p>
    pub fn set_deployment_instance_type(mut self, input: ::std::option::Option<crate::types::OptimizationJobDeploymentInstanceType>) -> Self {
        self.deployment_instance_type = input;
        self
    }
    /// <p>The type of instance that hosts the optimized model that you create with the optimization job.</p>
    pub fn get_deployment_instance_type(&self) -> &::std::option::Option<crate::types::OptimizationJobDeploymentInstanceType> {
        &self.deployment_instance_type
    }
    /// Adds a key-value pair to `optimization_environment`.
    ///
    /// To override the contents of this collection use [`set_optimization_environment`](Self::set_optimization_environment).
    ///
    /// <p>The environment variables to set in the model container.</p>
    pub fn optimization_environment(
        mut self,
        k: impl ::std::convert::Into<::std::string::String>,
        v: impl ::std::convert::Into<::std::string::String>,
    ) -> Self {
        let mut hash_map = self.optimization_environment.unwrap_or_default();
        hash_map.insert(k.into(), v.into());
        self.optimization_environment = ::std::option::Option::Some(hash_map);
        self
    }
    /// <p>The environment variables to set in the model container.</p>
    pub fn set_optimization_environment(
        mut self,
        input: ::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>>,
    ) -> Self {
        self.optimization_environment = input;
        self
    }
    /// <p>The environment variables to set in the model container.</p>
    pub fn get_optimization_environment(&self) -> &::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>> {
        &self.optimization_environment
    }
    /// Appends an item to `optimization_configs`.
    ///
    /// To override the contents of this collection use [`set_optimization_configs`](Self::set_optimization_configs).
    ///
    /// <p>Settings for each of the optimization techniques that the job applies.</p>
    pub fn optimization_configs(mut self, input: crate::types::OptimizationConfig) -> Self {
        let mut v = self.optimization_configs.unwrap_or_default();
        v.push(input);
        self.optimization_configs = ::std::option::Option::Some(v);
        self
    }
    /// <p>Settings for each of the optimization techniques that the job applies.</p>
    pub fn set_optimization_configs(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::OptimizationConfig>>) -> Self {
        self.optimization_configs = input;
        self
    }
    /// <p>Settings for each of the optimization techniques that the job applies.</p>
    pub fn get_optimization_configs(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::OptimizationConfig>> {
        &self.optimization_configs
    }
    /// <p>Details for where to store the optimized model that you create with the optimization job.</p>
    /// This field is required.
    pub fn output_config(mut self, input: crate::types::OptimizationJobOutputConfig) -> Self {
        self.output_config = ::std::option::Option::Some(input);
        self
    }
    /// <p>Details for where to store the optimized model that you create with the optimization job.</p>
    pub fn set_output_config(mut self, input: ::std::option::Option<crate::types::OptimizationJobOutputConfig>) -> Self {
        self.output_config = input;
        self
    }
    /// <p>Details for where to store the optimized model that you create with the optimization job.</p>
    pub fn get_output_config(&self) -> &::std::option::Option<crate::types::OptimizationJobOutputConfig> {
        &self.output_config
    }
    /// <p>Specifies a limit to how long a job can run. When the job reaches the time limit, SageMaker ends the job. Use this API to cap costs.</p>
    /// <p>To stop a training job, SageMaker sends the algorithm the <code>SIGTERM</code> signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost.</p>
    /// <p>The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with <code>CreateModel</code>.</p><note>
    /// <p>The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.</p>
    /// </note>
    /// This field is required.
    pub fn stopping_condition(mut self, input: crate::types::StoppingCondition) -> Self {
        self.stopping_condition = ::std::option::Option::Some(input);
        self
    }
    /// <p>Specifies a limit to how long a job can run. When the job reaches the time limit, SageMaker ends the job. Use this API to cap costs.</p>
    /// <p>To stop a training job, SageMaker sends the algorithm the <code>SIGTERM</code> signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost.</p>
    /// <p>The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with <code>CreateModel</code>.</p><note>
    /// <p>The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.</p>
    /// </note>
    pub fn set_stopping_condition(mut self, input: ::std::option::Option<crate::types::StoppingCondition>) -> Self {
        self.stopping_condition = input;
        self
    }
    /// <p>Specifies a limit to how long a job can run. When the job reaches the time limit, SageMaker ends the job. Use this API to cap costs.</p>
    /// <p>To stop a training job, SageMaker sends the algorithm the <code>SIGTERM</code> signal, which delays job termination for 120 seconds. Algorithms can use this 120-second window to save the model artifacts, so the results of training are not lost.</p>
    /// <p>The training algorithms provided by SageMaker automatically save the intermediate results of a model training job when possible. This attempt to save artifacts is only a best effort case as model might not be in a state from which it can be saved. For example, if training has just started, the model might not be ready to save. When saved, this intermediate data is a valid model artifact. You can use it to create a model with <code>CreateModel</code>.</p><note>
    /// <p>The Neural Topic Model (NTM) currently does not support saving intermediate model artifacts. When training NTMs, make sure that the maximum runtime is sufficient for the training job to complete.</p>
    /// </note>
    pub fn get_stopping_condition(&self) -> &::std::option::Option<crate::types::StoppingCondition> {
        &self.stopping_condition
    }
    /// Appends an item to `tags`.
    ///
    /// To override the contents of this collection use [`set_tags`](Self::set_tags).
    ///
    /// <p>A list of key-value pairs associated with the optimization job. For more information, see <a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">Tagging Amazon Web Services resources</a> in the <i>Amazon Web Services General Reference Guide</i>.</p>
    pub fn tags(mut self, input: crate::types::Tag) -> Self {
        let mut v = self.tags.unwrap_or_default();
        v.push(input);
        self.tags = ::std::option::Option::Some(v);
        self
    }
    /// <p>A list of key-value pairs associated with the optimization job. For more information, see <a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">Tagging Amazon Web Services resources</a> in the <i>Amazon Web Services General Reference Guide</i>.</p>
    pub fn set_tags(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::Tag>>) -> Self {
        self.tags = input;
        self
    }
    /// <p>A list of key-value pairs associated with the optimization job. For more information, see <a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">Tagging Amazon Web Services resources</a> in the <i>Amazon Web Services General Reference Guide</i>.</p>
    pub fn get_tags(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::Tag>> {
        &self.tags
    }
    /// <p>A VPC in Amazon VPC that your optimized model has access to.</p>
    pub fn vpc_config(mut self, input: crate::types::OptimizationVpcConfig) -> Self {
        self.vpc_config = ::std::option::Option::Some(input);
        self
    }
    /// <p>A VPC in Amazon VPC that your optimized model has access to.</p>
    pub fn set_vpc_config(mut self, input: ::std::option::Option<crate::types::OptimizationVpcConfig>) -> Self {
        self.vpc_config = input;
        self
    }
    /// <p>A VPC in Amazon VPC that your optimized model has access to.</p>
    pub fn get_vpc_config(&self) -> &::std::option::Option<crate::types::OptimizationVpcConfig> {
        &self.vpc_config
    }
    /// Consumes the builder and constructs a [`CreateOptimizationJobInput`](crate::operation::create_optimization_job::CreateOptimizationJobInput).
    pub fn build(
        self,
    ) -> ::std::result::Result<crate::operation::create_optimization_job::CreateOptimizationJobInput, ::aws_smithy_types::error::operation::BuildError>
    {
        ::std::result::Result::Ok(crate::operation::create_optimization_job::CreateOptimizationJobInput {
            optimization_job_name: self.optimization_job_name,
            role_arn: self.role_arn,
            model_source: self.model_source,
            deployment_instance_type: self.deployment_instance_type,
            optimization_environment: self.optimization_environment,
            optimization_configs: self.optimization_configs,
            output_config: self.output_config,
            stopping_condition: self.stopping_condition,
            tags: self.tags,
            vpc_config: self.vpc_config,
        })
    }
}
